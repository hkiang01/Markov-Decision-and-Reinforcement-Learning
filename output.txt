parsing grid
There are 6 rows and 6 columns
+ W +     + 
  -   + W - 
    -   +   
    S -   + 
  W W W -   
            
grid parsed
( 0 , 0 ):  1.8059204988
( 0 , 1 ):  2.77689094571
( 0 , 2 ):  1.8059204988
( 0 , 3 ):  1.78786129381
( 0 , 4 ):  0.8069006484
( 0 , 5 ):  0.798831641916
( 1 , 0 ):  2.77689094571
( 1 , 1 ):  1.8059204988
( 1 , 2 ):  1.78786129381
( 1 , 3 ):  0.8069006484
( 1 , 4 ):  0.798831641916
( 1 , 5 ):  -0.04
( 2 , 0 ):  1.8059204988
( 2 , 1 ):  2.77689094571
( 2 , 2 ):  1.8059204988
( 2 , 3 ):  0.874910845908
( 2 , 4 ):  -0.0792
( 2 , 5 ):  0.798831641916
( 3 , 0 ):  2.77689094571
( 3 , 1 ):  1.8059204988
( 3 , 2 ):  2.77689094571
( 3 , 3 ):  1.8059204988
( 3 , 4 ):  0.874910845908
( 3 , 5 ):  0.8069006484
( 4 , 0 ):  2.77689094571
( 4 , 1 ):  2.77689094571
( 4 , 2 ):  1.8059204988
( 4 , 3 ):  2.77689094571
( 4 , 4 ):  1.8059204988
( 4 , 5 ):  1.78786129381
( 5 , 0 ):  1.8059204988
( 5 , 1 ):  2.77689094571
( 5 , 2 ):  2.77689094571
( 5 , 3 ):  1.8059204988
( 5 , 4 ):  2.77689094571
( 5 , 5 ):  1.8059204988
printing grid...
1.0 	W	1.0 	0.0 	0.0 	1.0 	
0.0 	-1.0 	0.0 	1.0 	W	-1.0 	
0.0 	0.0 	-1.0 	0.0 	1.0 	0.0 	
0.0 	0.0 	S	-1.0 	0.0 	1.0 	
0.0 	W	W	W	-1.0 	0.0 	
0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	
grid printed
printing directions...
>	W	>	v	>	<	
^	^	^	^	W	^	
^	<	^	^	^	v	
^	^	^	^	^	^	
^	W	W	W	^	^	
^	v	>	>	>	^	
grid printed
printing utilities...
1.8059204988 	W	1.8059204988 	2.77689094571 	2.77689094571 	1.8059204988 	
2.77689094571 	1.8059204988 	2.77689094571 	1.8059204988 	W	2.77689094571 	
1.8059204988 	1.78786129381 	1.8059204988 	2.77689094571 	1.8059204988 	2.77689094571 	
1.78786129381 	0.8069006484 	0.874910845908 	1.8059204988 	2.77689094571 	1.8059204988 	
0.8069006484 	W	W	W	1.8059204988 	2.77689094571 	
0.798831641916 	-0.04 	0.798831641916 	0.8069006484 	1.78786129381 	1.8059204988 	
utilities printed
Moving to( 3 , 2 )
printing grid...
1.0 	W	1.0 	0.0 	0.0 	1.0 	
0.0 	-1.0 	0.0 	1.0 	W	-1.0 	
0.0 	0.0 	-1.0 	0.0 	1.0 	0.0 	
0.0 	0.0 	S	-1.0 	0.0 	1.0 	
0.0 	W	W	W	-1.0 	0.0 	
0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	
grid printed
[1.0]
TDLearning called on [ 0 , 0 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 1.0 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95]
[1.0, 0.95]
TDLearning called on [ 0 , 0 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95]
[1.0, 0.95, 0.95]
TDLearning called on [ 0 , 0 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 0 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 0 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 0 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 0 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 0 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 0 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 0 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0]
TDLearning called on [ 0 , 1 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 0 , 1 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 0 , 1 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 0 , 1 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 0 , 1 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 0 , 1 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 0 , 1 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 0 , 1 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 0 , 1 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 0 , 1 ] TD action called on wall, exiting
[0.0]
[1.0]
TDLearning called on [ 0 , 2 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 1.0 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95]
[1.0, 0.95]
TDLearning called on [ 0 , 2 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95]
[1.0, 0.95, 0.95]
TDLearning called on [ 0 , 2 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 2 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 2 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 2 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 2 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 2 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 2 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 2 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0]
TDLearning called on [ 0 , 3 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 0.0 1.0 1.0 1.0
action 1 qutil 0.0 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95]
[0.0, 0.95]
TDLearning called on [ 0 , 3 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 0.95 1.0 1.0 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95]
[0.0, 0.95, 0.95]
TDLearning called on [ 0 , 3 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 0.95 1.0 1.0 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 3 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 0.95 1.0 1.0 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 3 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 0.95 1.0 1.0 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 3 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 0.95 1.0 1.0 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 3 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 0.95 1 1.0 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 3 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 0.95 1 1.0 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 3 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 0.95 1 1.0 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 3 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 0.95 1 1.0 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0]
TDLearning called on [ 0 , 4 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 0.0 1.0 0.0 1.0
action 1 qutil 0.0 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95]
[0.0, 0.95]
TDLearning called on [ 0 , 4 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 0.95 1.0 0.95 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95]
[0.0, 0.95, 0.95]
TDLearning called on [ 0 , 4 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 0.95 1.0 0.95 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 4 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 0.95 1.0 0.95 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 4 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 0.95 1.0 0.95 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 4 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 0.95 1.0 0.95 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 4 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 0.95 1 0.95 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 4 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 0.95 1 0.95 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 4 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 0.95 1 0.95 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 4 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 0.95 1 0.95 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0]
TDLearning called on [ 0 , 5 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 1.0 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95]
[1.0, 0.95]
TDLearning called on [ 0 , 5 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95]
[1.0, 0.95, 0.95]
TDLearning called on [ 0 , 5 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 5 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 5 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 5 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 5 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 5 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 5 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 0 , 5 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0]
TDLearning called on [ 1 , 0 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 1.0 -1.0 0.0 0.0
action 0 qutil 0.0 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95]
[0.0, 0.95]
TDLearning called on [ 1 , 0 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 1.0 -1.0 0.0 0.95
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95]
[0.0, 0.95, 0.95]
TDLearning called on [ 1 , 0 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 1.0 -1.0 0.0 0.95
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 0 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 1.0 -1.0 0.0 0.95
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 0 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 1.0 -1.0 0.0 0.95
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 0 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 1.0 -1.0 0.0 0.95
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 0 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 1 -1.0 0.0 0.95
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 0 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 1 -1.0 0.0 0.95
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 0 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 1 -1.0 0.0 0.95
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 0 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 1 -1.0 0.0 0.95
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0]
TDLearning called on [ 1 , 1 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values -1.0 1.0 -1.0 1.0
action 1 qutil -1.0 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95]
[-1.0, 0.95]
TDLearning called on [ 1 , 1 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values -1.0 1.0 -1.0 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95]
[-1.0, 0.95, 0.95]
TDLearning called on [ 1 , 1 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values -1.0 1.0 -1.0 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 1 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values -1.0 1.0 -1.0 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 1 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values -1.0 1.0 -1.0 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 1 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values -1.0 1.0 -1.0 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 1 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values -1.0 1 -1.0 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 1 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values -1.0 1 -1.0 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 1 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values -1.0 1 -1.0 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 1 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values -1.0 1 -1.0 1.0
action 1 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0]
TDLearning called on [ 1 , 2 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 1.0 1.0 None -1.0
action 0 qutil 0.0 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95]
[0.0, 0.95]
TDLearning called on [ 1 , 2 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 1.0 1.0 None -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95]
[0.0, 0.95, 0.95]
TDLearning called on [ 1 , 2 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 1.0 1.0 None -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 2 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 1.0 1.0 None -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 2 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 1.0 1.0 None -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 2 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 1.0 1.0 None -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 2 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 1 1.0 None -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 2 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 1 1.0 None -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 2 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 1 1.0 None -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 2 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 1 1.0 None -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0]
TDLearning called on [ 1 , 3 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 1.0 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95]
[1.0, 0.95]
TDLearning called on [ 1 , 3 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95]
[1.0, 0.95, 0.95]
TDLearning called on [ 1 , 3 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 3 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 3 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 3 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 3 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 3 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 3 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 3 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0]
TDLearning called on [ 1 , 4 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 1 , 4 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 1 , 4 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 1 , 4 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 1 , 4 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 1 , 4 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 1 , 4 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 1 , 4 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 1 , 4 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 1 , 4 ] TD action called on wall, exiting
[0.0]
[-1.0]
TDLearning called on [ 1 , 5 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 1.0 -1.0 1.0 -1.0
action 0 qutil -1.0 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95]
[-1.0, 0.95]
TDLearning called on [ 1 , 5 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 1.0 -1.0 1.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95]
[-1.0, 0.95, 0.95]
TDLearning called on [ 1 , 5 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 1.0 -1.0 1.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 5 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 1.0 -1.0 1.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 5 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 1.0 -1.0 1.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 5 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 1.0 -1.0 1.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 5 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 1 -1.0 1.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 5 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 1 -1.0 1.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 5 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 1 -1.0 1.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 1 , 5 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 1 -1.0 1.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0]
TDLearning called on [ 2 , 0 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 1.0 -1.0 0.0 0.0
action 0 qutil 0.0 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95]
[0.0, 0.95]
TDLearning called on [ 2 , 0 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 1.0 -1.0 0.0 0.95
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95]
[0.0, 0.95, 0.95]
TDLearning called on [ 2 , 0 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 1.0 -1.0 0.0 0.95
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 0 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 1.0 -1.0 0.0 0.95
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 0 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 1.0 -1.0 0.0 0.95
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 0 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 1.0 -1.0 0.0 0.95
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 0 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 1 -1.0 0.0 0.95
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 0 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 1 -1.0 0.0 0.95
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 0 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 1 -1.0 0.0 0.95
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 0 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 1 -1.0 0.0 0.95
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0]
TDLearning called on [ 2 , 1 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values -1.0 None 0.0 0.95
action 3 qutil 0.0 QSPAP 0.95
new qutility: 0.9005
[0.0, 0.9005]
[0.0, 0.9005]
TDLearning called on [ 2 , 1 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values -1.0 None 0.0 0.95
action 3 qutil 0.9005 QSPAP 0.95
new qutility: 0.9005
[0.0, 0.9005, 0.9005]
[0.0, 0.9005, 0.9005]
TDLearning called on [ 2 , 1 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values -1.0 None 0.0 0.95
action 3 qutil 0.9005 QSPAP 0.95
new qutility: 0.9005
[0.0, 0.9005, 0.9005, 0.9005]
[0.0, 0.9005, 0.9005, 0.9005]
TDLearning called on [ 2 , 1 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values -1.0 None 0.0 0.95
action 3 qutil 0.9005 QSPAP 0.95
new qutility: 0.9005
[0.0, 0.9005, 0.9005, 0.9005, 0.9005]
[0.0, 0.9005, 0.9005, 0.9005, 0.9005]
TDLearning called on [ 2 , 1 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values -1.0 None 0.0 0.95
action 3 qutil 0.9005 QSPAP 0.95
new qutility: 0.9005
[0.0, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005]
[0.0, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005]
TDLearning called on [ 2 , 1 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values -1.0 None 0.0 0.95
action 3 qutil 0.9005 QSPAP 0.95
new qutility: 0.9005
[0.0, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005]
[0.0, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005]
TDLearning called on [ 2 , 1 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values -1.0 None 0.0 1
action 3 qutil 0.9005 QSPAP 0.95
new qutility: 0.9005
[0.0, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005]
[0.0, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005]
TDLearning called on [ 2 , 1 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values -1.0 None 0.0 1
action 3 qutil 0.9005 QSPAP 0.95
new qutility: 0.9005
[0.0, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005]
[0.0, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005]
TDLearning called on [ 2 , 1 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values -1.0 None 0.0 1
action 3 qutil 0.9005 QSPAP 0.95
new qutility: 0.9005
[0.0, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005]
[0.0, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005]
TDLearning called on [ 2 , 1 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values -1.0 None 0.0 1
action 3 qutil 0.9005 QSPAP 0.95
new qutility: 0.9005
[0.0, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005]
[-1.0]
TDLearning called on [ 2 , 2 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 1.0 1.0 0.0 -1.0
action 0 qutil -1.0 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95]
[-1.0, 0.95]
TDLearning called on [ 2 , 2 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 1.0 1.0 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95]
[-1.0, 0.95, 0.95]
TDLearning called on [ 2 , 2 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 1.0 1.0 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 2 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 1.0 1.0 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 2 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 1.0 1.0 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 2 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 1.0 1.0 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 2 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 1 1.0 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 2 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 1 1.0 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 2 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 1 1.0 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 2 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 1 1.0 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0]
TDLearning called on [ 2 , 3 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 1.0 1.0 -1.0 None
action 0 qutil 0.0 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95]
[0.0, 0.95]
TDLearning called on [ 2 , 3 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 1.0 1.0 -1.0 None
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95]
[0.0, 0.95, 0.95]
TDLearning called on [ 2 , 3 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 1.0 1.0 -1.0 None
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 3 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 1.0 1.0 -1.0 None
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 3 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 1.0 1.0 -1.0 None
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 3 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 1.0 1.0 -1.0 None
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 3 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 1 1.0 -1.0 None
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 3 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 1 1.0 -1.0 None
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 3 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 1 1.0 -1.0 None
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 3 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 1 1.0 -1.0 None
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0]
TDLearning called on [ 2 , 4 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 1.0 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95]
[1.0, 0.95]
TDLearning called on [ 2 , 4 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95]
[1.0, 0.95, 0.95]
TDLearning called on [ 2 , 4 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 4 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 4 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 4 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 4 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 4 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 4 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 4 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0]
TDLearning called on [ 2 , 5 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 1.0 0.0 1.0 1.0
action 0 qutil 0.0 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95]
[0.0, 0.95]
TDLearning called on [ 2 , 5 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 1.0 0.95 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95]
[0.0, 0.95, 0.95]
TDLearning called on [ 2 , 5 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 1.0 0.95 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 5 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 1.0 0.95 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 5 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 1.0 0.95 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 5 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 1.0 0.95 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 5 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 1 0.95 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 5 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 1 0.95 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 5 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 1 0.95 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 2 , 5 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 1 0.95 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0]
TDLearning called on [ 3 , 0 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 0.95 0.0 0.0 0.0
action 0 qutil 0.0 QSPAP 0.95
new qutility: 0.9005
[0.0, 0.9005]
[0.0, 0.9005]
TDLearning called on [ 3 , 0 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 0.95 0.0 0.0 0.9005
action 0 qutil 0.9005 QSPAP 0.95
new qutility: 0.9005
[0.0, 0.9005, 0.9005]
[0.0, 0.9005, 0.9005]
TDLearning called on [ 3 , 0 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 0.95 0.0 0.0 0.9005
action 0 qutil 0.9005 QSPAP 0.95
new qutility: 0.9005
[0.0, 0.9005, 0.9005, 0.9005]
[0.0, 0.9005, 0.9005, 0.9005]
TDLearning called on [ 3 , 0 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 0.95 0.0 0.0 0.9005
action 0 qutil 0.9005 QSPAP 0.95
new qutility: 0.9005
[0.0, 0.9005, 0.9005, 0.9005, 0.9005]
[0.0, 0.9005, 0.9005, 0.9005, 0.9005]
TDLearning called on [ 3 , 0 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 0.95 0.0 0.0 0.9005
action 0 qutil 0.9005 QSPAP 0.95
new qutility: 0.9005
[0.0, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005]
[0.0, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005]
TDLearning called on [ 3 , 0 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 0.95 0.0 0.0 0.9005
action 0 qutil 0.9005 QSPAP 0.95
new qutility: 0.9005
[0.0, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005]
[0.0, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005]
TDLearning called on [ 3 , 0 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 1 0.0 0.0 0.9005
action 0 qutil 0.9005 QSPAP 0.95
new qutility: 0.9005
[0.0, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005]
[0.0, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005]
TDLearning called on [ 3 , 0 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 1 0.0 0.0 0.9005
action 0 qutil 0.9005 QSPAP 0.95
new qutility: 0.9005
[0.0, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005]
[0.0, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005]
TDLearning called on [ 3 , 0 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 1 0.0 0.0 0.9005
action 0 qutil 0.9005 QSPAP 0.95
new qutility: 0.9005
[0.0, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005]
[0.0, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005]
TDLearning called on [ 3 , 0 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 1 0.0 0.0 0.9005
action 0 qutil 0.9005 QSPAP 0.95
new qutility: 0.9005
[0.0, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005, 0.9005]
[0.0]
TDLearning called on [ 3 , 1 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values -1.0 0.0 0.0 0.9005
action 3 qutil 0.0 QSPAP 0.9005
new qutility: 0.851495
[0.0, 0.8514949999999999]
[0.0, 0.8514949999999999]
TDLearning called on [ 3 , 1 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values -1.0 0.0 0.851495 0.9005
action 3 qutil 0.851495 QSPAP 0.9005
new qutility: 0.851495
[0.0, 0.8514949999999999, 0.8514949999999999]
[0.0, 0.8514949999999999, 0.8514949999999999]
TDLearning called on [ 3 , 1 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values -1.0 0.0 0.851495 0.9005
action 3 qutil 0.851495 QSPAP 0.9005
new qutility: 0.851495
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
TDLearning called on [ 3 , 1 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values -1.0 0.0 0.851495 0.9005
action 3 qutil 0.851495 QSPAP 0.9005
new qutility: 0.851495
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
TDLearning called on [ 3 , 1 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values -1.0 0.0 0.851495 0.9005
action 3 qutil 0.851495 QSPAP 0.9005
new qutility: 0.851495
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
TDLearning called on [ 3 , 1 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values -1.0 0.0 0.851495 0.9005
action 3 qutil 0.851495 QSPAP 0.9005
new qutility: 0.851495
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
TDLearning called on [ 3 , 1 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values -1.0 0.0 0.851495 1
action 3 qutil 0.851495 QSPAP 0.9005
new qutility: 0.851495
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
TDLearning called on [ 3 , 1 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values -1.0 0.0 0.851495 1
action 3 qutil 0.851495 QSPAP 0.9005
new qutility: 0.851495
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
TDLearning called on [ 3 , 1 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values -1.0 0.0 0.851495 1
action 3 qutil 0.851495 QSPAP 0.9005
new qutility: 0.851495
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
TDLearning called on [ 3 , 1 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values -1.0 0.0 0.851495 1
action 3 qutil 0.851495 QSPAP 0.9005
new qutility: 0.851495
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
[0.0]
TDLearning called on [ 3 , 2 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values None -1.0 0.0 0.851495
action 3 qutil 0.0 QSPAP 0.851495
new qutility: 0.80298005
[0.0, 0.8029800499999998]
[0.0, 0.8029800499999998]
TDLearning called on [ 3 , 2 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values None -1.0 0.80298005 0.851495
action 3 qutil 0.80298005 QSPAP 0.851495
new qutility: 0.80298005
[0.0, 0.8029800499999998, 0.8029800499999998]
[0.0, 0.8029800499999998, 0.8029800499999998]
TDLearning called on [ 3 , 2 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values None -1.0 0.80298005 0.851495
action 3 qutil 0.80298005 QSPAP 0.851495
new qutility: 0.80298005
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
TDLearning called on [ 3 , 2 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values None -1.0 0.80298005 0.851495
action 3 qutil 0.80298005 QSPAP 0.851495
new qutility: 0.80298005
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
TDLearning called on [ 3 , 2 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values None -1.0 0.80298005 0.851495
action 3 qutil 0.80298005 QSPAP 0.851495
new qutility: 0.80298005
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
TDLearning called on [ 3 , 2 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values None -1.0 0.80298005 0.851495
action 3 qutil 0.80298005 QSPAP 0.851495
new qutility: 0.80298005
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
TDLearning called on [ 3 , 2 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values None -1.0 0.80298005 1
action 3 qutil 0.80298005 QSPAP 0.851495
new qutility: 0.80298005
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
TDLearning called on [ 3 , 2 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values None -1.0 0.80298005 1
action 3 qutil 0.80298005 QSPAP 0.851495
new qutility: 0.80298005
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
TDLearning called on [ 3 , 2 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values None -1.0 0.80298005 1
action 3 qutil 0.80298005 QSPAP 0.851495
new qutility: 0.80298005
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
TDLearning called on [ 3 , 2 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values None -1.0 0.80298005 1
action 3 qutil 0.80298005 QSPAP 0.851495
new qutility: 0.80298005
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
[-1.0]
TDLearning called on [ 3 , 3 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 1.0 1.0 -1.0 0.80298005
action 0 qutil -1.0 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95]
[-1.0, 0.95]
TDLearning called on [ 3 , 3 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 1.0 1.0 -1.0 0.80298005
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95]
[-1.0, 0.95, 0.95]
TDLearning called on [ 3 , 3 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 1.0 1.0 -1.0 0.80298005
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95]
TDLearning called on [ 3 , 3 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 1.0 1.0 -1.0 0.80298005
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 3 , 3 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 1.0 1.0 -1.0 0.80298005
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 3 , 3 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 1.0 1.0 -1.0 0.80298005
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 3 , 3 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 1 1.0 -1.0 0.80298005
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 3 , 3 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 1 1.0 -1.0 0.80298005
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 3 , 3 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 1 1.0 -1.0 0.80298005
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 3 , 3 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 1 1.0 -1.0 0.80298005
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0]
TDLearning called on [ 3 , 4 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 1.0 1.0 -1.0 -1.0
action 0 qutil 0.0 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95]
[0.0, 0.95]
TDLearning called on [ 3 , 4 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 1.0 1.0 -1.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95]
[0.0, 0.95, 0.95]
TDLearning called on [ 3 , 4 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 1.0 1.0 -1.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95]
TDLearning called on [ 3 , 4 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 1.0 1.0 -1.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 3 , 4 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 1.0 1.0 -1.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 3 , 4 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 1.0 1.0 -1.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 3 , 4 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 1 1.0 -1.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 3 , 4 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 1 1.0 -1.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 3 , 4 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 1 1.0 -1.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 3 , 4 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 1 1.0 -1.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0]
TDLearning called on [ 3 , 5 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 1.0 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95]
[1.0, 0.95]
TDLearning called on [ 3 , 5 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95]
[1.0, 0.95, 0.95]
TDLearning called on [ 3 , 5 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95]
TDLearning called on [ 3 , 5 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 3 , 5 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 3 , 5 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 1.0 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 3 , 5 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 3 , 5 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 3 , 5 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 3 , 5 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 1 1.0 1.0 1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0]
TDLearning called on [ 4 , 0 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 0.9005 0.0 0.0 0.0
action 0 qutil 0.0 QSPAP 0.9005
new qutility: 0.851495
[0.0, 0.8514949999999999]
[0.0, 0.8514949999999999]
TDLearning called on [ 4 , 0 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 0.9005 0.851495 0.0 0.851495
action 0 qutil 0.851495 QSPAP 0.9005
new qutility: 0.851495
[0.0, 0.8514949999999999, 0.8514949999999999]
[0.0, 0.8514949999999999, 0.8514949999999999]
TDLearning called on [ 4 , 0 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 0.9005 0.851495 0.0 0.851495
action 0 qutil 0.851495 QSPAP 0.9005
new qutility: 0.851495
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
TDLearning called on [ 4 , 0 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 0.9005 0.851495 0.0 0.851495
action 0 qutil 0.851495 QSPAP 0.9005
new qutility: 0.851495
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
TDLearning called on [ 4 , 0 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 0.9005 0.851495 0.0 0.851495
action 0 qutil 0.851495 QSPAP 0.9005
new qutility: 0.851495
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
TDLearning called on [ 4 , 0 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 0.9005 0.851495 0.0 0.851495
action 0 qutil 0.851495 QSPAP 0.9005
new qutility: 0.851495
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
TDLearning called on [ 4 , 0 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 1 0.851495 0.0 0.851495
action 0 qutil 0.851495 QSPAP 0.9005
new qutility: 0.851495
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
TDLearning called on [ 4 , 0 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 1 0.851495 0.0 0.851495
action 0 qutil 0.851495 QSPAP 0.9005
new qutility: 0.851495
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
TDLearning called on [ 4 , 0 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 1 0.851495 0.0 0.851495
action 0 qutil 0.851495 QSPAP 0.9005
new qutility: 0.851495
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
TDLearning called on [ 4 , 0 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 1 0.851495 0.0 0.851495
action 0 qutil 0.851495 QSPAP 0.9005
new qutility: 0.851495
[0.0, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999, 0.8514949999999999]
[0.0]
TDLearning called on [ 4 , 1 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 1 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 1 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 1 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 1 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 1 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 1 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 1 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 1 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 1 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 2 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 2 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 2 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 2 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 2 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 2 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 2 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 2 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 2 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 2 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 3 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 3 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 3 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 3 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 3 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 3 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 3 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 3 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 3 ] TD action called on wall, exiting
[0.0]
[0.0]
TDLearning called on [ 4 , 3 ] TD action called on wall, exiting
[0.0]
[-1.0]
TDLearning called on [ 4 , 4 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 1.0 1.0 0.0 -1.0
action 0 qutil -1.0 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95]
[-1.0, 0.95]
TDLearning called on [ 4 , 4 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 1.0 1.0 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95]
[-1.0, 0.95, 0.95]
TDLearning called on [ 4 , 4 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 1.0 1.0 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95]
TDLearning called on [ 4 , 4 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 1.0 1.0 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 4 , 4 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 1.0 1.0 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 4 , 4 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 1.0 1.0 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 4 , 4 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 1 1.0 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 4 , 4 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 1 1.0 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 4 , 4 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 1 1.0 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 4 , 4 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 1 1.0 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[-1.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0]
TDLearning called on [ 4 , 5 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 1.0 0.0 0.0 -1.0
action 0 qutil 0.0 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95]
[0.0, 0.95]
TDLearning called on [ 4 , 5 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 1.0 0.95 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95]
[0.0, 0.95, 0.95]
TDLearning called on [ 4 , 5 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 1.0 0.95 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95]
TDLearning called on [ 4 , 5 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 1.0 0.95 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 4 , 5 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 1.0 0.95 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 4 , 5 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 1.0 0.95 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 4 , 5 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 1 0.95 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 4 , 5 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 1 0.95 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 4 , 5 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 1 0.95 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 4 , 5 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 1 0.95 0.0 -1.0
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0]
TDLearning called on [ 5 , 0 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 0.851495 0.0 0.0 0.0
action 0 qutil 0.0 QSPAP 0.851495
new qutility: 0.80298005
[0.0, 0.8029800499999998]
[0.0, 0.8029800499999998]
TDLearning called on [ 5 , 0 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 0.851495 0.0 0.80298005 0.80298005
action 0 qutil 0.80298005 QSPAP 0.851495
new qutility: 0.80298005
[0.0, 0.8029800499999998, 0.8029800499999998]
[0.0, 0.8029800499999998, 0.8029800499999998]
TDLearning called on [ 5 , 0 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 0.851495 0.0 0.80298005 0.80298005
action 0 qutil 0.80298005 QSPAP 0.851495
new qutility: 0.80298005
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
TDLearning called on [ 5 , 0 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 0.851495 0.0 0.80298005 0.80298005
action 0 qutil 0.80298005 QSPAP 0.851495
new qutility: 0.80298005
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
TDLearning called on [ 5 , 0 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 0.851495 0.0 0.80298005 0.80298005
action 0 qutil 0.80298005 QSPAP 0.851495
new qutility: 0.80298005
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
TDLearning called on [ 5 , 0 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 0.851495 0.0 0.80298005 0.80298005
action 0 qutil 0.80298005 QSPAP 0.851495
new qutility: 0.80298005
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
TDLearning called on [ 5 , 0 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 1 0.0 0.80298005 0.80298005
action 0 qutil 0.80298005 QSPAP 0.851495
new qutility: 0.80298005
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
TDLearning called on [ 5 , 0 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 1 0.0 0.80298005 0.80298005
action 0 qutil 0.80298005 QSPAP 0.851495
new qutility: 0.80298005
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
TDLearning called on [ 5 , 0 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 1 0.0 0.80298005 0.80298005
action 0 qutil 0.80298005 QSPAP 0.851495
new qutility: 0.80298005
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
TDLearning called on [ 5 , 0 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 1 0.0 0.80298005 0.80298005
action 0 qutil 0.80298005 QSPAP 0.851495
new qutility: 0.80298005
[0.0, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998, 0.8029800499999998]
[0.0]
TDLearning called on [ 5 , 1 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 0.0 0.0 0.0 0.80298005
action 3 qutil 0.0 QSPAP 0.80298005
new qutility: 0.7549502495
[0.0, 0.7549502494999998]
[0.0, 0.7549502494999998]
TDLearning called on [ 5 , 1 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 0.7549502495 0.0 0.7549502495 0.80298005
action 3 qutil 0.7549502495 QSPAP 0.80298005
new qutility: 0.7549502495
[0.0, 0.7549502494999998, 0.7549502494999998]
[0.0, 0.7549502494999998, 0.7549502494999998]
TDLearning called on [ 5 , 1 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 0.7549502495 0.0 0.7549502495 0.80298005
action 3 qutil 0.7549502495 QSPAP 0.80298005
new qutility: 0.7549502495
[0.0, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998]
[0.0, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998]
TDLearning called on [ 5 , 1 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 0.7549502495 0.0 0.7549502495 0.80298005
action 3 qutil 0.7549502495 QSPAP 0.80298005
new qutility: 0.7549502495
[0.0, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998]
[0.0, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998]
TDLearning called on [ 5 , 1 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 0.7549502495 0.0 0.7549502495 0.80298005
action 3 qutil 0.7549502495 QSPAP 0.80298005
new qutility: 0.7549502495
[0.0, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998]
[0.0, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998]
TDLearning called on [ 5 , 1 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 0.7549502495 0.0 0.7549502495 0.80298005
action 3 qutil 0.7549502495 QSPAP 0.80298005
new qutility: 0.7549502495
[0.0, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998]
[0.0, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998]
TDLearning called on [ 5 , 1 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 0.7549502495 0.0 0.7549502495 1
action 3 qutil 0.7549502495 QSPAP 0.80298005
new qutility: 0.7549502495
[0.0, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998]
[0.0, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998]
TDLearning called on [ 5 , 1 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 0.7549502495 0.0 0.7549502495 1
action 3 qutil 0.7549502495 QSPAP 0.80298005
new qutility: 0.7549502495
[0.0, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998]
[0.0, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998]
TDLearning called on [ 5 , 1 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 0.7549502495 0.0 0.7549502495 1
action 3 qutil 0.7549502495 QSPAP 0.80298005
new qutility: 0.7549502495
[0.0, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998]
[0.0, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998]
TDLearning called on [ 5 , 1 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 0.7549502495 0.0 0.7549502495 1
action 3 qutil 0.7549502495 QSPAP 0.80298005
new qutility: 0.7549502495
[0.0, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998, 0.7549502494999998]
[0.0]
TDLearning called on [ 5 , 2 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 0.0 0.0 0.0 0.7549502495
action 3 qutil 0.0 QSPAP 0.7549502495
new qutility: 0.707400747005
[0.0, 0.7074007470049998]
[0.0, 0.7074007470049998]
TDLearning called on [ 5 , 2 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 0.707400747005 0.0 0.707400747005 0.7549502495
action 3 qutil 0.707400747005 QSPAP 0.7549502495
new qutility: 0.707400747005
[0.0, 0.7074007470049998, 0.7074007470049998]
[0.0, 0.7074007470049998, 0.7074007470049998]
TDLearning called on [ 5 , 2 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 0.707400747005 0.0 0.707400747005 0.7549502495
action 3 qutil 0.707400747005 QSPAP 0.7549502495
new qutility: 0.707400747005
[0.0, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998]
[0.0, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998]
TDLearning called on [ 5 , 2 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 0.707400747005 0.0 0.707400747005 0.7549502495
action 3 qutil 0.707400747005 QSPAP 0.7549502495
new qutility: 0.707400747005
[0.0, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998]
[0.0, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998]
TDLearning called on [ 5 , 2 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 0.707400747005 0.0 0.707400747005 0.7549502495
action 3 qutil 0.707400747005 QSPAP 0.7549502495
new qutility: 0.707400747005
[0.0, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998]
[0.0, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998]
TDLearning called on [ 5 , 2 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 0.707400747005 0.0 0.707400747005 0.7549502495
action 3 qutil 0.707400747005 QSPAP 0.7549502495
new qutility: 0.707400747005
[0.0, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998]
[0.0, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998]
TDLearning called on [ 5 , 2 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 0.707400747005 0.0 0.707400747005 1
action 3 qutil 0.707400747005 QSPAP 0.7549502495
new qutility: 0.707400747005
[0.0, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998]
[0.0, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998]
TDLearning called on [ 5 , 2 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 0.707400747005 0.0 0.707400747005 1
action 3 qutil 0.707400747005 QSPAP 0.7549502495
new qutility: 0.707400747005
[0.0, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998]
[0.0, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998]
TDLearning called on [ 5 , 2 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 0.707400747005 0.0 0.707400747005 1
action 3 qutil 0.707400747005 QSPAP 0.7549502495
new qutility: 0.707400747005
[0.0, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998]
[0.0, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998]
TDLearning called on [ 5 , 2 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 0.707400747005 0.0 0.707400747005 1
action 3 qutil 0.707400747005 QSPAP 0.7549502495
new qutility: 0.707400747005
[0.0, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998, 0.7074007470049998]
[0.0]
TDLearning called on [ 5 , 3 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 0.0 0.0 0.0 0.707400747005
action 3 qutil 0.0 QSPAP 0.707400747005
new qutility: 0.660326739535
[0.0, 0.6603267395349497]
[0.0, 0.6603267395349497]
TDLearning called on [ 5 , 3 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 0.660326739535 0.0 0.660326739535 0.707400747005
action 3 qutil 0.660326739535 QSPAP 0.707400747005
new qutility: 0.660326739535
[0.0, 0.6603267395349497, 0.6603267395349497]
[0.0, 0.6603267395349497, 0.6603267395349497]
TDLearning called on [ 5 , 3 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 0.660326739535 0.0 0.660326739535 0.707400747005
action 3 qutil 0.660326739535 QSPAP 0.707400747005
new qutility: 0.660326739535
[0.0, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497]
[0.0, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497]
TDLearning called on [ 5 , 3 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 0.660326739535 0.0 0.660326739535 0.707400747005
action 3 qutil 0.660326739535 QSPAP 0.707400747005
new qutility: 0.660326739535
[0.0, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497]
[0.0, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497]
TDLearning called on [ 5 , 3 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 0.660326739535 0.0 0.660326739535 0.707400747005
action 3 qutil 0.660326739535 QSPAP 0.707400747005
new qutility: 0.660326739535
[0.0, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497]
[0.0, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497]
TDLearning called on [ 5 , 3 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 0.660326739535 0.0 0.660326739535 0.707400747005
action 3 qutil 0.660326739535 QSPAP 0.707400747005
new qutility: 0.660326739535
[0.0, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497]
[0.0, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497]
TDLearning called on [ 5 , 3 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 0.660326739535 0.0 0.660326739535 1
action 3 qutil 0.660326739535 QSPAP 0.707400747005
new qutility: 0.660326739535
[0.0, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497]
[0.0, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497]
TDLearning called on [ 5 , 3 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 0.660326739535 0.0 0.660326739535 1
action 3 qutil 0.660326739535 QSPAP 0.707400747005
new qutility: 0.660326739535
[0.0, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497]
[0.0, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497]
TDLearning called on [ 5 , 3 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 0.660326739535 0.0 0.660326739535 1
action 3 qutil 0.660326739535 QSPAP 0.707400747005
new qutility: 0.660326739535
[0.0, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497]
[0.0, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497]
TDLearning called on [ 5 , 3 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 0.660326739535 0.0 0.660326739535 1
action 3 qutil 0.660326739535 QSPAP 0.707400747005
new qutility: 0.660326739535
[0.0, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497, 0.6603267395349497]
[0.0]
TDLearning called on [ 5 , 4 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values -1.0 0.0 0.0 0.660326739535
action 3 qutil 0.0 QSPAP 0.660326739535
new qutility: 0.61372347214
[0.0, 0.6137234721396001]
[0.0, 0.6137234721396001]
TDLearning called on [ 5 , 4 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values -1.0 0.0 0.61372347214 0.660326739535
action 3 qutil 0.61372347214 QSPAP 0.660326739535
new qutility: 0.61372347214
[0.0, 0.6137234721396001, 0.6137234721396001]
[0.0, 0.6137234721396001, 0.6137234721396001]
TDLearning called on [ 5 , 4 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values -1.0 0.0 0.61372347214 0.660326739535
action 3 qutil 0.61372347214 QSPAP 0.660326739535
new qutility: 0.61372347214
[0.0, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001]
[0.0, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001]
TDLearning called on [ 5 , 4 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values -1.0 0.0 0.61372347214 0.660326739535
action 3 qutil 0.61372347214 QSPAP 0.660326739535
new qutility: 0.61372347214
[0.0, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001]
[0.0, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001]
TDLearning called on [ 5 , 4 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values -1.0 0.0 0.61372347214 0.660326739535
action 3 qutil 0.61372347214 QSPAP 0.660326739535
new qutility: 0.61372347214
[0.0, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001]
[0.0, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001]
TDLearning called on [ 5 , 4 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values -1.0 0.0 0.61372347214 0.660326739535
action 3 qutil 0.61372347214 QSPAP 0.660326739535
new qutility: 0.61372347214
[0.0, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001]
[0.0, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001]
TDLearning called on [ 5 , 4 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values -1.0 0.0 0.61372347214 1
action 3 qutil 0.61372347214 QSPAP 0.660326739535
new qutility: 0.61372347214
[0.0, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001]
[0.0, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001]
TDLearning called on [ 5 , 4 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values -1.0 0.0 0.61372347214 1
action 3 qutil 0.61372347214 QSPAP 0.660326739535
new qutility: 0.61372347214
[0.0, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001]
[0.0, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001]
TDLearning called on [ 5 , 4 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values -1.0 0.0 0.61372347214 1
action 3 qutil 0.61372347214 QSPAP 0.660326739535
new qutility: 0.61372347214
[0.0, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001]
[0.0, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001]
TDLearning called on [ 5 , 4 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values -1.0 0.0 0.61372347214 1
action 3 qutil 0.61372347214 QSPAP 0.660326739535
new qutility: 0.61372347214
[0.0, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001, 0.6137234721396001]
[0.0]
TDLearning called on [ 5 , 5 ] TDHelper t (timestep) 1
TDHelper alpha 1.0
candidateActions values 1.0 0.0 0.0 0.61372347214
action 0 qutil 0.0 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95]
[0.0, 0.95]
TDLearning called on [ 5 , 5 ] TDHelper t (timestep) 2
TDHelper alpha 0.983606557377
candidateActions values 1.0 0.95 0.95 0.61372347214
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95]
[0.0, 0.95, 0.95]
TDLearning called on [ 5 , 5 ] TDHelper t (timestep) 3
TDHelper alpha 0.967741935484
candidateActions values 1.0 0.95 0.95 0.61372347214
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95]
TDLearning called on [ 5 , 5 ] TDHelper t (timestep) 4
TDHelper alpha 0.952380952381
candidateActions values 1.0 0.95 0.95 0.61372347214
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 5 , 5 ] TDHelper t (timestep) 5
TDHelper alpha 0.9375
candidateActions values 1.0 0.95 0.95 0.61372347214
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 5 , 5 ] TDHelper t (timestep) 6
TDHelper alpha 0.923076923077
candidateActions values 1.0 0.95 0.95 0.61372347214
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 5 , 5 ] TDHelper t (timestep) 7
TDHelper alpha 0.909090909091
fun condition met, returning reward function
candidateActions values 1 0.95 0.95 0.61372347214
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 5 , 5 ] TDHelper t (timestep) 8
TDHelper alpha 0.89552238806
fun condition met, returning reward function
candidateActions values 1 0.95 0.95 0.61372347214
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 5 , 5 ] TDHelper t (timestep) 9
TDHelper alpha 0.882352941176
fun condition met, returning reward function
candidateActions values 1 0.95 0.95 0.61372347214
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
TDLearning called on [ 5 , 5 ] TDHelper t (timestep) 10
TDHelper alpha 0.869565217391
fun condition met, returning reward function
candidateActions values 1 0.95 0.95 0.61372347214
action 0 qutil 0.95 QSPAP 1.0
new qutility: 0.95
[0.0, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
printing utilities...
0.95 	W	0.95 	0.95 	0.95 	0.95 	
0.95 	0.95 	0.95 	0.95 	W	0.95 	
0.95 	0.9005 	0.95 	0.95 	0.95 	0.95 	
0.9005 	0.851495 	0.80298005 	0.95 	0.95 	0.95 	
0.851495 	W	W	W	0.95 	0.95 	
0.80298005 	0.7549502495 	0.707400747005 	0.660326739535 	0.61372347214 	0.95 	
qutilities printed
printing RMSErrors...
0.851496375535 	W	0.851496375535 	1.93264840897 	1.93264840897 	0.851496375535 	
1.93264840897 	1.17547626124 	1.93264840897 	0.851496375535 	W	2.0810882313 	
0.981063794175 	1.00320163221 	1.17547626124 	1.93264840897 	0.851496375535 	1.93264840897 	
1.00320163221 	0.246977219843 	0.272565151892 	1.17547626124 	1.93264840897 	0.851496375535 	
0.246977219843 	W	W	W	1.17547626124 	1.93264840897 	
0.240889277663 	0.758051269063 	0.256147698438 	0.280572104488 	1.24252105337 	0.981063794175 	
RMSErrors printed
